Meta:
  num_objs: 2  # Two objectives

Environment:
  dimensions: [20, 20]
  ep_length: 25
  timestep_penalty: 0
  global_reward_mode: "Aggregated"   # Reward can be scored at every timestep
  local_reward_mode: "exponential"   # (Used mostly if you look at local rewards, but not crucial)
  local_reward_kneecap: 10.0
  local_reward_temp: 2
  observation_mode: 'density'
  poi_obs_temp: 2
  agent_obs_temp: 2
  include_location_in_obs: False     # Agents do NOT see their own absolute position

  # POIs: each must be within 'radius'=1, with 1 rover (coupling=1)
  # to yield a reward of 1. Observations can be done at any timestep [0..14].
  pois:
    - obj: 0
      location: [3, 3]
      radius: 1
      coupling: 1
      obs_window: [0, 100]
      reward: 1.5
      repeat: True

    - obj: 1
      location: [5, 15]
      radius: 1
      coupling: 1
      obs_window: [0, 100]
      reward: 1.5
      repeat: True

Agents:
  # One rover (the first dimension's length => 1 agent)
  starting_locs: [[2, 2]]
  num_sensors: [4]
  observation_radii: [5]       # They can "see" POIs up to distance 5
  max_step_sizes: [1]   # Slows movement, requiring careful planning
